{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e4adc3a99414f8dbfc0b378770a71a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ab6cee4fcee4d9199280d1387e48a75",
              "IPY_MODEL_a4d84028b50c416e8a7f6c441315b74e",
              "IPY_MODEL_663a5e57bcab4ed683e7b010a63087ec"
            ],
            "layout": "IPY_MODEL_057a909e917446ffb0fc97e597753235"
          }
        },
        "6ab6cee4fcee4d9199280d1387e48a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50fd739cc051403a822cee13abcef656",
            "placeholder": "​",
            "style": "IPY_MODEL_8cba47fd87c14981b8b0097445ab091e",
            "value": "Batches: 100%"
          }
        },
        "a4d84028b50c416e8a7f6c441315b74e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f63663fcd4748f98c2840594b957721",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c15dc75c6c94db7a7bb934fe97e8a0c",
            "value": 1
          }
        },
        "663a5e57bcab4ed683e7b010a63087ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3054ef8ff99c410886f9199dacd34241",
            "placeholder": "​",
            "style": "IPY_MODEL_2b0a3f7011ee4421b66dee16ec38950d",
            "value": " 1/1 [00:01&lt;00:00,  1.98s/it]"
          }
        },
        "057a909e917446ffb0fc97e597753235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50fd739cc051403a822cee13abcef656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cba47fd87c14981b8b0097445ab091e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f63663fcd4748f98c2840594b957721": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c15dc75c6c94db7a7bb934fe97e8a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3054ef8ff99c410886f9199dacd34241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b0a3f7011ee4421b66dee16ec38950d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYm-pvP0RQ6g",
        "outputId": "c44703f8-ba8b-4585-9721-511bee36f5cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.12/dist-packages (4.4.7)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n"
          ]
        }
      ],
      "source": [
        "#Install Requirements\n",
        "!pip install reportlab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 1 — Create 8 Rich PDFs (2–3 Pages Each)\n",
        "\n",
        "import os\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, ListFlowable, ListItem\n",
        "from reportlab.lib.enums import TA_CENTER\n",
        "\n",
        "BASE_DIR = \"rag_papers\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "def create_pdf(filepath, title, abstract, sections):\n",
        "    doc = SimpleDocTemplate(\n",
        "        filepath, pagesize=A4,\n",
        "        rightMargin=36, leftMargin=36, topMargin=36, bottomMargin=36\n",
        "    )\n",
        "    styles = getSampleStyleSheet()\n",
        "\n",
        "    title_style = ParagraphStyle(\n",
        "        name=\"TitleStyle\",\n",
        "        parent=styles[\"Title\"],\n",
        "        alignment=TA_CENTER,\n",
        "        fontSize=18,\n",
        "        spaceAfter=14\n",
        "    )\n",
        "    header_style = ParagraphStyle(\n",
        "        name=\"HeaderStyle\",\n",
        "        parent=styles[\"Heading2\"],\n",
        "        fontSize=13,\n",
        "        spaceAfter=8,\n",
        "        spaceBefore=14\n",
        "    )\n",
        "    body_style = ParagraphStyle(\n",
        "        name=\"BodyStyle\",\n",
        "        parent=styles[\"BodyText\"],\n",
        "        fontSize=10.5,\n",
        "        leading=14,\n",
        "        spaceAfter=6\n",
        "    )\n",
        "\n",
        "    flow = []\n",
        "    flow.append(Paragraph(title, title_style))\n",
        "    flow.append(Spacer(1, 12))\n",
        "\n",
        "    flow.append(Paragraph(\"<b>Abstract</b>\", header_style))\n",
        "    flow.append(Paragraph(abstract, body_style))\n",
        "    flow.append(Spacer(1, 14))\n",
        "\n",
        "    for sec_title, sec_text in sections:\n",
        "        flow.append(Paragraph(sec_title, header_style))\n",
        "        for para in sec_text.split(\"\\n\"):\n",
        "            flow.append(Paragraph(para.strip(), body_style))\n",
        "        flow.append(Spacer(1, 10))\n",
        "\n",
        "    doc.build(flow)\n"
      ],
      "metadata": {
        "id": "EDj04qu0RWpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 2 — Generate 8 Sector-Based Research PDFs\n",
        "papers = [\n",
        "    {\n",
        "        \"filename\": \"RAG_Foundations.pdf\",\n",
        "        \"title\": \"Foundations of Retrieval-Augmented Generation (RAG)\",\n",
        "        \"abstract\": \"\"\"Retrieval-Augmented Generation (RAG) combines information retrieval with neural text generation to overcome\n",
        "the limitations of parametric memory in large language models. This paper presents a comprehensive study of RAG,\n",
        "including its architecture, document preprocessing strategies, indexing mechanisms, and answer generation pipelines.\"\"\",\n",
        "        \"sections\": [\n",
        "            (\"1. Introduction\",\n",
        "             \"Large Language Models (LLMs) rely primarily on internal parameters for knowledge storage. While powerful, \"\n",
        "             \"this paradigm limits their ability to provide up-to-date and verifiable information.\\n\"\n",
        "             \"Retrieval-Augmented Generation introduces an external knowledge source that can be dynamically queried.\\n\"\n",
        "             \"This approach enables factual grounding, reduces hallucination, and supports domain-specific question answering.\"),\n",
        "            (\"2. Architecture of RAG\",\n",
        "             \"A typical RAG pipeline consists of five stages: document ingestion, text chunking, embedding generation, \"\n",
        "             \"vector indexing, and answer synthesis.\\n\"\n",
        "             \"Each component contributes to retrieval accuracy and response relevance.\"),\n",
        "            (\"3. Interaction Between Components\",\n",
        "             \"The user query is embedded and compared against document embeddings stored in a vector index.\\n\"\n",
        "             \"Top-k relevant chunks are retrieved and injected into the prompt for the LLM.\\n\"\n",
        "             \"The LLM generates an answer strictly grounded in the retrieved context.\"),\n",
        "            (\"4. Benefits and Limitations\",\n",
        "             \"RAG improves explainability and factual consistency but depends heavily on retrieval quality.\\n\"\n",
        "             \"Suboptimal chunking or embedding strategies can lead to missing or misleading evidence.\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"filename\": \"Transformer_Architecture.pdf\",\n",
        "        \"title\": \"Transformer Architecture: Encoder, Decoder, and Sub-Layers\",\n",
        "        \"abstract\": \"\"\"The Transformer model forms the backbone of modern NLP systems. This paper explains the architecture,\n",
        "including encoder and decoder stacks, self-attention mechanisms, and feed-forward sub-layers.\"\"\",\n",
        "        \"sections\": [\n",
        "            (\"1. Overview\",\n",
        "             \"The Transformer replaces recurrent and convolutional layers with self-attention.\\n\"\n",
        "             \"This enables parallel processing and improved modeling of long-range dependencies.\"),\n",
        "            (\"2. Encoder Layer\",\n",
        "             \"Each encoder layer consists of two sub-layers: (1) Multi-Head Self-Attention and (2) Position-wise Feed-Forward Network.\\n\"\n",
        "             \"Residual connections and layer normalization are applied around each sub-layer.\"),\n",
        "            (\"3. Decoder Layer\",\n",
        "             \"The decoder contains three sub-layers: masked self-attention, encoder-decoder attention, and feed-forward networks.\\n\"\n",
        "             \"Masking ensures autoregressive behavior during training and inference.\"),\n",
        "            (\"4. Training and Inference\",\n",
        "             \"Transformers allow efficient batch training.\\n\"\n",
        "             \"During inference, tokens are generated sequentially in an autoregressive manner.\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"filename\": \"Positional_Encoding.pdf\",\n",
        "        \"title\": \"Positional Encoding in Transformer Models\",\n",
        "        \"abstract\": \"\"\"Since Transformers do not use recurrence, they require positional encodings to preserve word order.\n",
        "This paper explores sinusoidal and learned positional encoding techniques.\"\"\",\n",
        "        \"sections\": [\n",
        "            (\"1. Motivation\",\n",
        "             \"Self-attention mechanisms are permutation-invariant.\\n\"\n",
        "             \"Without positional information, word order is lost.\"),\n",
        "            (\"2. Sinusoidal Encoding\",\n",
        "             \"Sinusoidal encodings use periodic functions of different frequencies to represent token positions.\\n\"\n",
        "             \"They enable extrapolation to longer sequences.\"),\n",
        "            (\"3. Learned Position Embeddings\",\n",
        "             \"Position embeddings can also be learned during training.\\n\"\n",
        "             \"While flexible, they may generalize poorly to unseen sequence lengths.\"),\n",
        "            (\"4. Importance\",\n",
        "             \"Positional encoding allows the model to learn syntax, sequence order, and hierarchical structures in language.\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"filename\": \"Multi_Head_Attention.pdf\",\n",
        "        \"title\": \"Multi-Head Attention in Transformer Architecture\",\n",
        "        \"abstract\": \"\"\"Multi-head attention enhances self-attention by enabling the model to attend to multiple representation\n",
        "subspaces simultaneously. This paper explains its design and benefits.\"\"\",\n",
        "        \"sections\": [\n",
        "            (\"1. Self-Attention Recap\",\n",
        "             \"Self-attention computes interactions between all tokens in a sequence.\"),\n",
        "            (\"2. Multi-Head Mechanism\",\n",
        "             \"Multiple attention heads operate on different linear projections of the input.\\n\"\n",
        "             \"Outputs are concatenated and projected.\"),\n",
        "            (\"3. Advantages\",\n",
        "             \"Allows the model to capture syntactic, semantic, and long-range dependencies simultaneously.\"),\n",
        "            (\"4. Applications\",\n",
        "             \"Used in machine translation, summarization, and large-scale language modeling.\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"filename\": \"Few_Shot_Learning.pdf\",\n",
        "        \"title\": \"Few-Shot Learning in Large Language Models\",\n",
        "        \"abstract\": \"\"\"Few-shot learning enables LLMs to perform tasks using only a small number of examples.\n",
        "This paper explains in-context learning and its advantages.\"\"\",\n",
        "        \"sections\": [\n",
        "            (\"1. Definition\",\n",
        "             \"Few-shot learning refers to task generalization from minimal examples provided at inference time.\"),\n",
        "            (\"2. In-Context Learning\",\n",
        "             \"LLMs infer patterns directly from examples embedded in the prompt.\"),\n",
        "            (\"3. Benefits\",\n",
        "             \"Eliminates the need for task-specific fine-tuning.\"),\n",
        "            (\"4. Challenges\",\n",
        "             \"Performance depends on prompt design and context length.\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"filename\": \"GPT3_Inference.pdf\",\n",
        "        \"title\": \"Inference Strategies in GPT-3\",\n",
        "        \"abstract\": \"\"\"GPT-3 popularized in-context learning and prompt-based inference. This paper describes decoding strategies\n",
        "and few-shot prompting mechanisms.\"\"\",\n",
        "        \"sections\": [\n",
        "            (\"1. Autoregressive Generation\",\n",
        "             \"GPT-3 generates text token-by-token conditioned on previous context.\"),\n",
        "            (\"2. Prompt Engineering\",\n",
        "             \"Tasks are described using natural language instructions and examples.\"),\n",
        "            (\"3. Sampling Techniques\",\n",
        "             \"Top-k, nucleus sampling, and temperature control output diversity.\"),\n",
        "            (\"4. Few-Shot Inference\",\n",
        "             \"Multiple examples are embedded in the prompt to guide generation.\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"filename\": \"Vector_Search_and_ANN.pdf\",\n",
        "        \"title\": \"Vector Search and Approximate Nearest Neighbor Algorithms\",\n",
        "        \"abstract\": \"\"\"This paper discusses vector similarity search, indexing structures, and approximate nearest neighbor (ANN)\n",
        "algorithms such as FAISS and HNSW.\"\"\",\n",
        "        \"sections\": [\n",
        "            (\"1. Vector Representations\",\n",
        "             \"Text is mapped into high-dimensional embedding spaces.\"),\n",
        "            (\"2. Exact vs Approximate Search\",\n",
        "             \"Exact search is computationally expensive.\\n\"\n",
        "             \"ANN methods trade small accuracy loss for large performance gains.\"),\n",
        "            (\"3. FAISS and HNSW\",\n",
        "             \"FAISS uses clustering and quantization.\\n\"\n",
        "             \"HNSW builds navigable small-world graphs.\"),\n",
        "            (\"4. Applications\",\n",
        "             \"Semantic search, recommendation systems, and RAG pipelines.\")\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"filename\": \"Hybrid_Retrieval.pdf\",\n",
        "        \"title\": \"Hybrid Retrieval: Combining Dense and Sparse Search\",\n",
        "        \"abstract\": \"\"\"Hybrid retrieval combines semantic embeddings with keyword-based search to improve recall and precision.\n",
        "This paper explains hybrid ranking strategies in information retrieval.\"\"\",\n",
        "        \"sections\": [\n",
        "            (\"1. Sparse Retrieval\",\n",
        "             \"Keyword-based methods such as BM25 rely on term frequency and inverse document frequency.\"),\n",
        "            (\"2. Dense Retrieval\",\n",
        "             \"Embedding-based methods capture semantic similarity.\"),\n",
        "            (\"3. Hybrid Methods\",\n",
        "             \"Scores from dense and sparse retrieval are combined using weighted ranking.\"),\n",
        "            (\"4. Use in RAG\",\n",
        "             \"Hybrid search improves retrieval robustness for complex user queries.\")\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "for paper in papers:\n",
        "    path = os.path.join(BASE_DIR, paper[\"filename\"])\n",
        "    create_pdf(path, paper[\"title\"], paper[\"abstract\"], paper[\"sections\"])\n",
        "    print(f\"Created: {path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egok_iPjRYvT",
        "outputId": "8248ac50-ed4d-4914-a6d2-e479476f4393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created: rag_papers/RAG_Foundations.pdf\n",
            "Created: rag_papers/Transformer_Architecture.pdf\n",
            "Created: rag_papers/Positional_Encoding.pdf\n",
            "Created: rag_papers/Multi_Head_Attention.pdf\n",
            "Created: rag_papers/Few_Shot_Learning.pdf\n",
            "Created: rag_papers/GPT3_Inference.pdf\n",
            "Created: rag_papers/Vector_Search_and_ANN.pdf\n",
            "Created: rag_papers/Hybrid_Retrieval.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2 — End-to-End RAG System Using Groq\n",
        "!pip install langchain faiss-cpu sentence-transformers pypdf scikit-learn groq\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdE1JPwPRZ7M",
        "outputId": "877e5e44-2e91-44ae-af8d-6c3029df8843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.6)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.4.59)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-community langchain-text-splitters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7gaKQrYRcgw",
        "outputId": "cd275de0-0af4-46cf-ee6f-7aec864cf0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.6)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.59)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 3 — Load PDFs\n",
        "from pypdf import PdfReader\n",
        "\n",
        "def load_pdfs(pdf_dir):\n",
        "    documents = []\n",
        "    for fname in os.listdir(pdf_dir):\n",
        "        if fname.endswith(\".pdf\"):\n",
        "            path = os.path.join(pdf_dir, fname)\n",
        "            reader = PdfReader(path)\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "            documents.append({\"source\": fname, \"text\": text})\n",
        "    return documents\n",
        "\n",
        "docs = load_pdfs(\"rag_papers\")\n",
        "print(\"Loaded documents:\", len(docs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez4P0WyIRh4R",
        "outputId": "078451f0-c235-4151-afe5-8689038c61ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded documents: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 4 — Chunking Strategy\n",
        "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=700,\n",
        "    chunk_overlap=120\n",
        ")\n",
        "\n",
        "chunks = []\n",
        "for doc in docs:\n",
        "    pieces = splitter.split_text(doc[\"text\"])\n",
        "    for i, p in enumerate(pieces):\n",
        "        chunks.append({\n",
        "            \"source\": doc[\"source\"],\n",
        "            \"chunk_id\": i,\n",
        "            \"text\": p\n",
        "        })\n",
        "\n",
        "print(\"Total chunks:\", len(chunks))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n81rSLdURlJY",
        "outputId": "9b6f96cb-9887-436c-8eea-73dea74d1a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "texts = [c[\"text\"] for c in chunks]\n",
        "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "print(\"Total vectors indexed:\", index.ntotal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "6e4adc3a99414f8dbfc0b378770a71a0",
            "6ab6cee4fcee4d9199280d1387e48a75",
            "a4d84028b50c416e8a7f6c441315b74e",
            "663a5e57bcab4ed683e7b010a63087ec",
            "057a909e917446ffb0fc97e597753235",
            "50fd739cc051403a822cee13abcef656",
            "8cba47fd87c14981b8b0097445ab091e",
            "0f63663fcd4748f98c2840594b957721",
            "7c15dc75c6c94db7a7bb934fe97e8a0c",
            "3054ef8ff99c410886f9199dacd34241",
            "2b0a3f7011ee4421b66dee16ec38950d"
          ]
        },
        "id": "BkCUNTR4RlL8",
        "outputId": "8fbeca52-ba8e-43f0-b673-afaf2e372a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e4adc3a99414f8dbfc0b378770a71a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total vectors indexed: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step converts all text chunks into numerical vectors and stores them in a FAISS index so that similar content can be retrieved efficiently.\n",
        "\n",
        "Cell for keyword based retrieval and hybrid search"
      ],
      "metadata": {
        "id": "9605Wn83Rq7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "tfidf_matrix = tfidf.fit_transform(texts)\n",
        "\n",
        "def hybrid_search(query, top_k=5):\n",
        "    query_embedding = embedding_model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    query_tfidf = tfidf.transform([query])\n",
        "    keyword_scores = (tfidf_matrix @ query_tfidf.T).toarray().ravel()\n",
        "\n",
        "    combined_indices = set(indices[0])\n",
        "    top_keywords = np.argsort(keyword_scores)[-top_k:]\n",
        "    combined_indices = combined_indices.union(set(top_keywords))\n",
        "\n",
        "    results = []\n",
        "    for idx in combined_indices:\n",
        "        dense_score = 0\n",
        "        if idx in indices[0]:\n",
        "            dense_score = -distances[0][list(indices[0]).index(idx)]\n",
        "        sparse_score = keyword_scores[idx]\n",
        "        final_score = dense_score + sparse_score\n",
        "        results.append((idx, final_score))\n",
        "\n",
        "    results = sorted(results, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "CskX8ddJRrif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import pickle\n",
        "\n",
        "faiss.write_index(index, \"rag_vector_index.faiss\")\n",
        "\n",
        "with open(\"rag_metadata.pkl\", \"wb\") as f:\n",
        "    pickle.dump(chunks, f)\n",
        "\n",
        "print(\"Vector database saved to disk\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# import faiss\n",
        "# import pickle\n",
        "\n",
        "# index = faiss.read_index(\"rag_vector_index.faiss\")\n",
        "\n",
        "# with open(\"rag_metadata.pkl\", \"rb\") as f:\n",
        "#     chunks = pickle.load(f)\n",
        "\n",
        "# print(\"Vector database loaded successfully\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obbaMnoYRvg1",
        "outputId": "9f21b273-071e-4784-d59c-db74176215ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector database saved to disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step allows both semantic search using embeddings and keyword matching using TF IDF.\n",
        "\n",
        "Cell for GORQ LLM integration using llama-3.1-8b-instant"
      ],
      "metadata": {
        "id": "-Q7NxC7ERx0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=\"\")"
      ],
      "metadata": {
        "id": "OkkH4fHmRyR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query):\n",
        "    retrieved_chunks = hybrid_search(query, top_k=5)\n",
        "\n",
        "    context_text = \"\"\n",
        "    sources = []\n",
        "\n",
        "    for idx, _ in retrieved_chunks:\n",
        "        context_text += chunks[idx][\"text\"] + \"\\n\\n\"\n",
        "        sources.append(chunks[idx][\"source\"])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant. Answer the question using only the context below.\n",
        "Do not use external knowledge.\n",
        "\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    answer = completion.choices[0].message.content\n",
        "    return answer, sources\n"
      ],
      "metadata": {
        "id": "G5X_2y6WR2kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What are the main components of a RAG model and how do they interact?\",\n",
        "    \"What are the two sub layers in each encoder layer of the Transformer?\",\n",
        "    \"Explain how positional encoding is implemented and why it is necessary\",\n",
        "    \"Describe multi head attention and why it is beneficial\"\n",
        "\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    answer, sources = generate_answer(q)\n",
        "    print(\"Question:\", q)\n",
        "    print(\"Answer:\", answer)\n",
        "    print(\"Sources:\", sources)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysl-tRTfR5eq",
        "outputId": "a5bde072-b1fa-4d3c-de66-f1488820237c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the main components of a RAG model and how do they interact?\n",
            "Answer: According to the text, a typical RAG pipeline consists of five stages: \n",
            "\n",
            "1. Document ingestion\n",
            "2. Text chunking\n",
            "3. Embedding generation\n",
            "4. Vector indexing\n",
            "5. Answer synthesis\n",
            "\n",
            "The user query is embedded and compared against document embeddings stored in a vector index. The top-k relevant chunks are retrieved and injected into the prompt for the LLM. The LLM then generates an answer strictly grounded in the retrieved context.\n",
            "Sources: ['Multi_Head_Attention.pdf', 'RAG_Foundations.pdf', 'RAG_Foundations.pdf', 'RAG_Foundations.pdf', 'Positional_Encoding.pdf']\n",
            "\n",
            "Question: What are the two sub layers in each encoder layer of the Transformer?\n",
            "Answer: The two sub-layers in each encoder layer of the Transformer are:\n",
            "\n",
            "1. Multi-Head Self-Attention\n",
            "2. Position-wise Feed-Forward Network.\n",
            "Sources: ['Few_Shot_Learning.pdf', 'Transformer_Architecture.pdf', 'Transformer_Architecture.pdf', 'Positional_Encoding.pdf', 'Multi_Head_Attention.pdf']\n",
            "\n",
            "Question: Explain how positional encoding is implemented and why it is necessary\n",
            "Answer: Positional encoding is a technique used in Transformer models to preserve word order and sequence structure in language. It is necessary because self-attention mechanisms are permutation-invariant, meaning they do not take into account the order of the input tokens.\n",
            "\n",
            "There are two main techniques for implementing positional encoding:\n",
            "\n",
            "1. **Sinusoidal Encoding**: This method uses periodic functions of different frequencies to represent token positions. The sinusoidal encoding is calculated as a function of the position in the sequence, and it is added to the input embeddings. This allows the model to learn the syntax, sequence order, and hierarchical structures in language.\n",
            "\n",
            "2. **Learned Position Embeddings**: In this method, position embeddings are learned during training. The position embeddings are added to the input embeddings, and the model learns to use them to capture the positional information.\n",
            "\n",
            "Positional encoding is necessary because without it, the self-attention mechanisms would treat all input tokens as equivalent, regardless of their position in the sequence. This would result in the loss of word order and sequence structure, making it difficult for the model to learn meaningful representations of language. By incorporating positional encoding, the model can learn to capture the relationships between tokens in a sequence, and generate coherent and contextually relevant outputs.\n",
            "Sources: ['Few_Shot_Learning.pdf', 'Transformer_Architecture.pdf', 'Positional_Encoding.pdf', 'Positional_Encoding.pdf', 'Transformer_Architecture.pdf']\n",
            "\n",
            "Question: Describe multi head attention and why it is beneficial\n",
            "Answer: Multi-head attention is a mechanism that enhances self-attention by enabling the model to attend to multiple representation subspaces simultaneously. This is achieved by having multiple attention heads operate on different linear projections of the input. \n",
            "\n",
            "In other words, instead of a single attention head processing the entire input, multiple attention heads process different aspects of the input, such as different features or dimensions. The outputs from each attention head are then concatenated and projected, allowing the model to capture a more comprehensive understanding of the input.\n",
            "\n",
            "Multi-head attention is beneficial because it allows the model to capture syntactic, semantic, and long-range dependencies simultaneously. This is particularly useful in natural language processing tasks, where understanding the relationships between different words and phrases is crucial. By using multiple attention heads, the model can focus on different aspects of the input, such as the meaning of individual words, the relationships between words, and the overall structure of the sentence. This enables the model to capture a more nuanced and accurate understanding of the input, leading to improved performance in tasks such as machine translation, summarization, and large-scale language modeling.\n",
            "Sources: ['Multi_Head_Attention.pdf', 'Transformer_Architecture.pdf', 'Transformer_Architecture.pdf', 'Few_Shot_Learning.pdf', 'Positional_Encoding.pdf']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_query = input(\"Enter your question or type exit to stop: \")\n",
        "\n",
        "    if user_query.lower() == \"exit\":\n",
        "        print(\"Session ended\")\n",
        "        break\n",
        "\n",
        "    answer, sources = generate_answer(user_query)\n",
        "\n",
        "    print(\"\\nAnswer:\\n\", answer)\n",
        "    print(\"\\nSources:\\n\", sources)\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqoisrvRR8Bj",
        "outputId": "719714fa-ee04-44cd-f0e3-04012a41433d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question or type exit to stop: What is few shot learning and how does GPT three implement it during inference\n",
            "\n",
            "Answer:\n",
            " Few-shot learning refers to task generalization from minimal examples provided at inference time. It enables Large Language Models (LLMs) to perform tasks using only a small number of examples.\n",
            "\n",
            "GPT-3 implements few-shot learning through in-context learning, where the model infers patterns directly from examples embedded in the prompt. This approach eliminates the need for task-specific fine-tuning. During inference, GPT-3 uses prompt engineering to describe tasks using natural language instructions and examples, and then generates text token-by-token conditioned on previous context. The model also employs sampling techniques, such as top-k, nucleus sampling, and temperature control, to output diverse responses.\n",
            "\n",
            "Sources:\n",
            " ['Multi_Head_Attention.pdf', 'Transformer_Architecture.pdf', 'GPT3_Inference.pdf', 'Few_Shot_Learning.pdf', 'Transformer_Architecture.pdf']\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Enter your question or type exit to stop: exit\n",
            "Session ended\n"
          ]
        }
      ]
    }
  ]
}