{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Smart Quiz Generator from Study Material**"
      ],
      "metadata": {
        "id": "58n_1tRcVPJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Study Assistant for Automated Quiz Generation**\n",
        "Project Overview:This project implements a lightweight AI-powered study assistant that helps students summarize educational content and automatically generate multiple-choice quiz questions for self-assessment. The system is designed to be simple, fast, and standalone, without relying on external databases, vector storage, or complex frameworks."
      ],
      "metadata": {
        "id": "7nyovw39UU06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Required Libraries\n",
        "!pip install reportlab PyPDF2 groq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWVxhGn2PMXv",
        "outputId": "8798bd7a-6e2c-4b8e-9584-aad308b45012"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: reportlab in /usr/local/lib/python3.12/dist-packages (4.4.7)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Create a Multi-Page Study PDF\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "pdf_file = \"prompt_engineering_study_material.pdf\"\n",
        "\n",
        "content_pages = [\n",
        "    \"\"\"\n",
        "    Topic: Introduction to Prompt Engineering\n",
        "\n",
        "    Prompt Engineering is the practice of designing and optimizing inputs to large language models (LLMs)\n",
        "    to obtain accurate, relevant, and structured outputs. It controls how AI models interpret user intent.\n",
        "\n",
        "    A prompt may include instructions, context, constraints, examples, and formatting rules.\n",
        "    Well-written prompts improve output quality and reduce hallucinations.\n",
        "    \"\"\",\n",
        "\n",
        "    \"\"\"\n",
        "    Topic: Types of Prompts\n",
        "\n",
        "    Zero-shot Prompting:\n",
        "    Asking without examples.\n",
        "\n",
        "    Few-shot Prompting:\n",
        "    Providing a few examples to guide the model.\n",
        "\n",
        "    Chain-of-Thought Prompting:\n",
        "    Asking the model to show reasoning steps.\n",
        "\n",
        "    These techniques improve reasoning and reduce ambiguity.\n",
        "    \"\"\",\n",
        "\n",
        "    \"\"\"\n",
        "    Topic: Prompt Elements and Patterns\n",
        "\n",
        "    Prompt elements:\n",
        "    - Instruction\n",
        "    - Context\n",
        "    - Input Data\n",
        "    - Output Format\n",
        "\n",
        "    Prompt patterns:\n",
        "    - Question-Answer Pattern\n",
        "    - Role-based Prompting\n",
        "    - Few-shot Examples\n",
        "    - Step-by-step Reasoning\n",
        "    \"\"\",\n",
        "\n",
        "    \"\"\"\n",
        "    Topic: What to Avoid in Prompt Design\n",
        "\n",
        "    - Ambiguous instructions\n",
        "    - Overly long prompts\n",
        "    - Conflicting constraints\n",
        "    - Unclear output format\n",
        "\n",
        "    Poor prompts lead to hallucinations, inconsistency, and incomplete answers.\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "c = canvas.Canvas(pdf_file, pagesize=A4)\n",
        "\n",
        "for page_text in content_pages:\n",
        "    text_object = c.beginText(40, 800)\n",
        "    for line in page_text.split(\"\\n\"):\n",
        "        text_object.textLine(line)\n",
        "    c.drawText(text_object)\n",
        "    c.showPage()\n",
        "\n",
        "c.save()\n",
        "\n",
        "pdf_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J2O4w2yiQS4k",
        "outputId": "df3db6b9-133c-44fd-adcd-15b290f5e8ef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'prompt_engineering_study_material.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Extract Text from PDF\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def extract_pdf_text(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "pdf_text = extract_pdf_text(\"prompt_engineering_study_material.pdf\")\n",
        "print(pdf_text[:1000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur1DU_ivQfg6",
        "outputId": "34462e7f-2d2c-4f37-d885-74d2b12c5cab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Topic: Introduction to Prompt Engineering\n",
            "    Prompt Engineering is the practice of designing and optimizing inputs to large language models (LLMs) \n",
            "    to obtain accurate, relevant, and structured outputs. It controls how AI models interpret user intent.\n",
            "    A prompt may include instructions, context, constraints, examples, and formatting rules.\n",
            "    Well-written prompts improve output quality and reduce hallucinations.\n",
            "    \n",
            "\n",
            "    Topic: Types of Prompts\n",
            "    Zero-shot Prompting:\n",
            "    Asking without examples.\n",
            "    Few-shot Prompting:\n",
            "    Providing a few examples to guide the model.\n",
            "    Chain-of-Thought Prompting:\n",
            "    Asking the model to show reasoning steps.\n",
            "    These techniques improve reasoning and reduce ambiguity.\n",
            "    \n",
            "\n",
            "    Topic: Prompt Elements and Patterns\n",
            "    Prompt elements:\n",
            "    - Instruction\n",
            "    - Context\n",
            "    - Input Data\n",
            "    - Output Format\n",
            "    Prompt patterns:\n",
            "    - Question-Answer Pattern\n",
            "    - Role-based Prompting\n",
            "    - Few-shot Examples\n",
            "    - Step-by-step Reasoning\n",
            "    \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup Groq API\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=\"key\")\n"
      ],
      "metadata": {
        "id": "8epWTPP1QtMQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Summarize the PDF (Groq LLM)\n",
        "def summarize_text(text):\n",
        "    prompt = f\"\"\"\n",
        "You are a study assistant.\n",
        "\n",
        "Summarize the following educational material into concise bullet points.\n",
        "Ensure full topic coverage and avoid adding new information.\n",
        "\n",
        "Study Material:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",   # Supported model\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.3,\n",
        "        max_tokens=800\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "summary = summarize_text(pdf_text)\n",
        "print(\"===== SUMMARY =====\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMI2i7KoRP2L",
        "outputId": "dda7ad14-8313-4900-fe05-ad9dfeb43a91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== SUMMARY =====\n",
            "Here are the concise bullet points summarizing the educational material on Introduction to Prompt Engineering:\n",
            "\n",
            "**Introduction to Prompt Engineering**\n",
            "\n",
            "* Prompt Engineering is the practice of designing and optimizing inputs to large language models (LLMs) to obtain accurate, relevant, and structured outputs.\n",
            "* Well-written prompts improve output quality and reduce hallucinations.\n",
            "* A prompt may include instructions, context, constraints, examples, and formatting rules.\n",
            "\n",
            "**Types of Prompts**\n",
            "\n",
            "* **Zero-shot Prompting**: Asking without examples.\n",
            "* **Few-shot Prompting**: Providing a few examples to guide the model.\n",
            "* **Chain-of-Thought Prompting**: Asking the model to show reasoning steps.\n",
            "\n",
            "**Prompt Elements and Patterns**\n",
            "\n",
            "* **Prompt Elements**:\n",
            "  - Instruction\n",
            "  - Context\n",
            "  - Input Data\n",
            "  - Output Format\n",
            "* **Prompt Patterns**:\n",
            "  - Question-Answer Pattern\n",
            "  - Role-based Prompting\n",
            "  - Few-shot Examples\n",
            "  - Step-by-step Reasoning\n",
            "\n",
            "**What to Avoid in Prompt Design**\n",
            "\n",
            "* **Ambiguous instructions**\n",
            "* **Overly long prompts**\n",
            "* **Conflicting constraints**\n",
            "* **Unclear output format**\n",
            "* Poor prompts lead to hallucinations, inconsistency, and incomplete answers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5 â€“ MCQ Generation (Fixed)\n",
        "def generate_mcqs(summary):\n",
        "    prompt = f\"\"\"\n",
        "You are an educational quiz generator.\n",
        "\n",
        "Based strictly on the following summary, generate 5 multiple-choice questions.\n",
        "Each question must:\n",
        "- Have 4 options (A, B, C, D)\n",
        "- Clearly indicate the correct answer\n",
        "- Be conceptually diverse\n",
        "- Avoid adding new information\n",
        "\n",
        "Summary:\n",
        "{summary}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",   # Supported model\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.4,\n",
        "        max_tokens=1200\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "quiz = generate_mcqs(summary)\n",
        "print(\"===== QUIZ =====\")\n",
        "print(quiz)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2cnDK4QSBtT",
        "outputId": "5a9da99d-0598-4260-b5bc-ad72cc214c5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== QUIZ =====\n",
            "Here are 5 multiple-choice questions based on the provided summary:\n",
            "\n",
            "**Question 1:** What is the primary goal of Prompt Engineering in the context of large language models (LLMs)?\n",
            "\n",
            "A) To increase the computational power of LLMs\n",
            "B) To improve output quality and reduce hallucinations\n",
            "C) To decrease the size of LLMs\n",
            "D) To enhance the user interface of LLMs\n",
            "\n",
            "**Correct Answer:** B) To improve output quality and reduce hallucinations\n",
            "\n",
            "**Question 2:** Which type of prompting involves asking the model to show reasoning steps?\n",
            "\n",
            "A) Zero-shot Prompting\n",
            "B) Few-shot Prompting\n",
            "C) Chain-of-Thought Prompting\n",
            "D) Role-based Prompting\n",
            "\n",
            "**Correct Answer:** C) Chain-of-Thought Prompting\n",
            "\n",
            "**Question 3:** What is an example of a prompt element that provides information about the expected output format?\n",
            "\n",
            "A) Instruction\n",
            "B) Context\n",
            "C) Input Data\n",
            "D) Output Format\n",
            "\n",
            "**Correct Answer:** D) Output Format\n",
            "\n",
            "**Question 4:** What is a common issue that arises from poorly designed prompts?\n",
            "\n",
            "A) Overly short prompts\n",
            "B) Hallucinations, inconsistency, and incomplete answers\n",
            "C) Slow response times\n",
            "D) Limited model capabilities\n",
            "\n",
            "**Correct Answer:** B) Hallucinations, inconsistency, and incomplete answers\n",
            "\n",
            "**Question 5:** Which prompt pattern involves providing a few examples to guide the model?\n",
            "\n",
            "A) Question-Answer Pattern\n",
            "B) Role-based Prompting\n",
            "C) Few-shot Examples\n",
            "D) Step-by-step Reasoning\n",
            "\n",
            "**Correct Answer:** C) Few-shot Examples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mcqs(summary):\n",
        "    prompt = f\"\"\"\n",
        "You are an educational quiz generator.\n",
        "\n",
        "Based strictly on the following summary, generate 5 multiple-choice questions.\n",
        "Each question must:\n",
        "- Have 4 options (A, B, C, D)\n",
        "- Clearly indicate the correct answer\n",
        "- Be conceptually diverse\n",
        "- Avoid adding new information\n",
        "\n",
        "Summary:\n",
        "{summary}\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.4\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "quiz = generate_mcqs(summary)\n",
        "print(\"===== QUIZ =====\")\n",
        "print(quiz)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3EfTTRbSBv2",
        "outputId": "1f18cd16-281e-4c49-ace8-e26a6ac8a9d3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== QUIZ =====\n",
            "Here are 5 multiple-choice questions based on the provided summary:\n",
            "\n",
            "**Question 1:** What is the primary goal of Prompt Engineering?\n",
            "\n",
            "A) To increase the computational power of large language models (LLMs)\n",
            "B) To design and optimize inputs to LLMs for accurate outputs\n",
            "C) To reduce the training time of LLMs\n",
            "D) To improve the memory capacity of LLMs\n",
            "\n",
            "**Correct Answer:** B) To design and optimize inputs to LLMs for accurate outputs\n",
            "\n",
            "**Question 2:** Which type of prompting involves asking the model to show reasoning steps?\n",
            "\n",
            "A) Zero-shot Prompting\n",
            "B) Few-shot Prompting\n",
            "C) Chain-of-Thought Prompting\n",
            "D) Role-based Prompting\n",
            "\n",
            "**Correct Answer:** C) Chain-of-Thought Prompting\n",
            "\n",
            "**Question 3:** What is a common problem that occurs when prompts are poorly designed?\n",
            "\n",
            "A) Hallucinations and inconsistency in output\n",
            "B) Increased computational power of LLMs\n",
            "C) Improved output quality and reduced hallucinations\n",
            "D) Faster training time of LLMs\n",
            "\n",
            "**Correct Answer:** A) Hallucinations and inconsistency in output\n",
            "\n",
            "**Question 4:** Which of the following is an example of a prompt element?\n",
            "\n",
            "A) Output Format\n",
            "B) Instruction\n",
            "C) Both A and B\n",
            "D) None of the above\n",
            "\n",
            "**Correct Answer:** C) Both A and B\n",
            "\n",
            "**Question 5:** What should be avoided in prompt design to minimize hallucinations and inconsistency?\n",
            "\n",
            "A) Overly long prompts\n",
            "B) Ambiguous instructions\n",
            "C) Both A and B\n",
            "D) None of the above\n",
            "\n",
            "**Correct Answer:** C) Both A and B\n"
          ]
        }
      ]
    }
  ]
}